# Proje-1: Regresyon

Bu proje iki aÅŸamadan oluÅŸmaktadÄ±r. Ä°lk kÄ±sÄ±m lineer regresyon kullanarak, bir evin iki Ã¶zelliÄŸi ile fiyatÄ±nÄ± tahmin etmeye yÃ¶neliktir. Ä°kinci kÄ±sÄ±m ise evin bir Ã§ok Ã¶zelliÄŸini, Ã§oklu lineer regresyon kullanarak fiyatÄ±nÄ± tahmin etmeye yÃ¶neliktir. 

Direk kodlarÄ± incelemek iÃ§in odev1.py ve odev2.py dosyalarÄ±nÄ± inceleyebilirsin. EÄŸer notlar ile beraber incelemek istersen proje_1_Regresyon.ipynb dosyasÄ±nÄ± inceleyebilirsin.


Verileri indirmek iÃ§in : <a href="kc_house_data.csv">kc_house_data.csv</a>


Bu rapor Notion kullanarak yazÄ±lmÄ±ÅŸtÄ±r. Buradan Notion sayfasÄ±na ulaÅŸabilir ve raporu daha dÃ¼zenli okuyabilirsiniz.


<img src="https://upload.wikimedia.org/wikipedia/commons/4/45/Notion_app_logo.png" alt="https://upload.wikimedia.org/wikipedia/commons/4/45/Notion_app_logo.png" width="30px" /> Link : [Notion](https://www.notion.so/Proje-1-Regresyon-08c0dec28ff349439db3dfe26f1b0b5d)


TÃ¼m iÅŸlemler google colab kullanarak yapÄ±ldÄ±. Kodlara ve .ipynb dosyasÄ±na ulaÅŸmak iÃ§in : 


<img src="https://cdn-icons-png.flaticon.com/512/25/25231.png" alt="https://cdn-icons-png.flaticon.com/512/25/25231.png" width="30px" /> Link : [Proje 1 - Regresyon](https://github.com/Pilestin/My_ML_Adventure/tree/master/Makine%20%C3%96%C4%9Frenmesine%20Giri%C5%9F/Proje%201%20-%20Regresyon)


Veri Seti : **`https://www.kaggle.com/datasets/harlfoxem/housesalesprediction`**

## a) *YaÅŸam alanÄ± (sqft_living) ve arsa (sqft_lot) Ã¶zniteliklerini (feature, attribute) kullanarak evin fiyatÄ±nÄ± tahmin edecek basit bir regresyon modeli geliÅŸtiriniz.*

- Ä°lk olarak yapmamÄ±z gerekenler gerekli kÃ¼tÃ¼phaneleri import etmek olacak. Tabi ki modelimi eÄŸiteceÄŸimiz algoritmamÄ±zÄ± kendimiz yazacaÄŸÄ±z. Fakat verileri uygun veri yapÄ±larÄ±nda tutmayÄ±, gÃ¶rselleÅŸtirmeyi veya parÃ§alamayÄ± uygun kÃ¼tÃ¼phaneleri kullanarak yapacaÄŸÄ±z.

BaÅŸlangÄ±Ã§ olarak nump, pandas ve maplotlib kÃ¼tÃ¼phanelerini import ettik. Yeri geldiÄŸinde bir kaÃ§ import iÅŸlemi daha yapÄ±lacak 



![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled.png)

Verilerimizi .csv dosyasÄ±ndan almamÄ±z gerek. Bunun iÃ§in pandas kÃ¼tÃ¼phanesinden read_csv metodunu kullanÄ±yoruz. Buradan bir pandas data frame tipinde nesne dÃ¶necektir ve bu nesne tÃ¼m tabloyu tutmakta. 

Ä°lk olarak bu data frame nesnesinin shape( boyut ) bilgisine bakÄ±yoruz. 

ArdÄ±ndan head ile ilk 5 satÄ±rÄ± gÃ¶rmekteyiz.

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%201.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%202.png)

- Verilerimizi inceledikten sonra artÄ±k train ve test olarak ayÄ±rma iÅŸlemine geÃ§ebiliriz.

Ã–ncelikle ayÄ±rma iÅŸlemi iÃ§in sklearn.model_selection iÃ§erisinden train_test_split metodunu import ettik. X â€˜e attributeâ€™larÄ±mÄ±zÄ± aldÄ±k. Y ise Ã§Ä±kÄ±ÅŸ deÄŸeri olan fiyat biligisini tutacak. 

Bu metodun iÃ§erisine X , Y ve train_size bilgisini verip verilerimizi ayÄ±rdÄ±k. 

x_train ve y_train ile verilerimizi eÄŸÄŸiteceÄŸiz. x_test ve y_test ile ise doÄŸrulama ve maliyet deÄŸerlerimizi kontrol edeceÄŸiz.

Burada x ve y parÃ§alarÄ±mÄ±zÄ±n her biri pandas Dataframe tipinde.  

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%203.png)

- SÄ±radaki iÅŸlemimize geÃ§meden Ã¶nce verilerimize normalizasyon uygulamalÄ±yÄ±z. Aksi halde verilerimizi eÄŸitmemiz Ã§ok daha uzun sÃ¼recektir.

Bu iÅŸlemi kendimiz de yazabilirdik fakat burada hazÄ±r bulunan sklearn.preprocessing iÃ§erisindeki MinMaxScaler kullanÄ±mÄ± tercih edildi.

Arkaplanda aÅŸaÄŸÄ±daki formÃ¼l her x deÄŸeri iÃ§in uygulanmakta. 

<aside>
ğŸ“ $(X - min(x)) / ( max(x) - mix(x))$

</aside>

Normalize edilen veirler ise tekrar x_train ve x_test deÄŸiÅŸkenlerine verildi. 

Åu anda bu deÄŸiÅŸkenler **numpy.ndaray** tipinde. BunlarÄ±n Ã¼zerinde iÅŸlem yapmamÄ±z artÄ±k daha kolay olacak. 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%204.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%205.png)

X verileri 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%206.png)

Y verileri 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%207.png)

- Åimdi elimizde hazÄ±r verilerimiz bulunduÄŸuna gÃ¶re kendi fonksiyonlarÄ±mÄ±zÄ± yazabiliriz. Ä°lk olarak maliyet fonksiyonumuzu gÃ¶relim.

### Maliyet Fonksiyonu - 1

- Maliyet fonksiyonumuz x ve y aralarÄ±ndaki hata oranÄ±nÄ± bize gÃ¶stermekteydi. AyrÄ±ca optimumâ€™a ne kadar yaklaÅŸtÄ±ÄŸÄ±mÄ±zÄ± bulabilmekteydik.
1. soru iÃ§in J fonksiyonumuz sadece Q0 , Q1 , Q2 deÄŸerleri ile Ã§alÄ±ÅŸacaÄŸÄ± iÃ§in direk bu parametreleri alabilir. Fakat 2. soruda bunlarÄ± bir liste iÃ§erisinde tutacaÄŸÄ±z.

Bu fonksiyon ilk olarak bir toplam deÄŸiÅŸkeni belirlemekte. ArdÄ±ndan X â€˜in boyutu kadar dÃ¶ngÃ¼de parantez iÃ§erisinde verilen iÅŸlemi uygulamakta.

Bu iÅŸlem aslÄ±nda Hq(x) hipotez deÄŸerimizdir. Bunu 2. soruda ayrÄ± bir fonksiyon olarak ayÄ±racaÄŸÄ±z fakat ÅŸu an iÃ§in bu ÅŸekilde devam edilecek.

Bu hipotez ile doÄŸrusal bir eÄŸri uydurmuÅŸ ve bu eÄŸri ile gerÃ§ek Y deÄŸeri arasÄ±ndaki farkÄ± bulmuÅŸ oluruz. Yani hatamÄ±z. ArdÄ±ndan bu deÄŸerlerin her satÄ±r iÃ§in bulunup karelerini alarak toplamaktayÄ±z.

Son olarak ise 1/2m yani veri sayÄ±sÄ±nÄ±n 2 katÄ±na sonucumuzu bÃ¶lmekteyiz. 

Maliyet fonksiyonumuz tamamlandÄ±. 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%208.png)

- Åimdi gradient descent iÃ§erisinde kullandÄ±ÄŸÄ±mÄ±z maliyet fonksiyonunun tÃ¼revini yapan fonksiyonu yazabiliriz.

Burada gÃ¶rdÃ¼klerimiz de yukarÄ±da yaptÄ±ÄŸÄ±mÄ±z iÅŸlemlerin neredeyse aynÄ±sÄ±. FarkÄ±mÄ±z tÃ¼rev alÄ±ndÄ±ÄŸÄ± iÃ§in kare bÃ¶lÃ¼mÃ¼ Ã§arpan olarak gelip 1/2m deÄŸerini 1/m â€˜e Ã§evirmekte. 

AyrÄ±ca parantez iÃ§erisinin de tÃ¼revi alÄ±nacaÄŸÄ± iÃ§in ( Dikkat Qâ€™ya gÃ¶re tÃ¼rev alÄ±nmakta. Bilinmeyen Q ) Qâ€™larÄ±n Ã¶nÃ¼ndeki katsayÄ±lar Ã§arpan olarak gelmekte.

Son olarakta toplam deÄŸer 1/m ile Ã§arpÄ±lmakta.

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%209.png)

- Åimdi asÄ±l eÄŸitme aÅŸamasÄ±nÄ± yapacaÄŸÄ±mÄ±z GradientDescent algoritmamÄ±zÄ± yazabiliriz. Ama Ã¶ncelikle Q deÄŸerlerimizi oluÅŸturalÄ±m.

BaÅŸlangÄ±Ã§ iÃ§in Q deÄŸerlerini 1,1,1 seÃ§ebiliriz. Fakat bu sayÄ±lar optimum deÄŸerlere doÄŸru deÄŸiÅŸeceÄŸi iÃ§in doÄŸru seÃ§im yapmak Ã¶nemlidir. Ä°lk uygulamada Q deÄŸerleri bazÄ± sayÄ±lara yakÄ±nsamaktadÄ±r. Bu sayÄ±larÄ±n yakÄ±n olduÄŸu deÄŸerleri vererek eÄŸitme aÅŸamasÄ±nÄ± kÄ±saltmayÄ± ve maliyet deÄŸerlerini dÃ¼ÅŸÃ¼rmeyi hedefleyebiliriz. 

AyrÄ±ca cost listesi oluÅŸturarak her iterasyonda maliyetin ne kadar olduÄŸunu tutabiliriz.

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2010.png)

### GradientDescent AlgoritmasÄ±

Åimdi tÃ¼m parametrelerimizi deÄŸiÅŸtirdiÄŸimiz ve maliyeti gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z aÅŸamaya geldik.

Burada ilk olarak bir epoch deÄŸerimiz var. Bu deÄŸer kadar xâ€™leri eÄŸitmekteyiz. 

ArdÄ±ndan her adÄ±mda bir maliyet fonksiyonunu kullanarak hatamÄ±zÄ± cost listesi iÃ§erisine atmaktayÄ±z. Burada hatalarÄ±mÄ±zÄ± ezberleme olmamasÄ± iÃ§in x_test ve y_test ile hesaplamaktayÄ±z.

SonrasÄ±nda her Q deÄŸerini q0,q1,q2 gibi deÄŸiÅŸkenlerde tutarak hesaplamaktayÄ±z. Bunu Q deÄŸerinden alfa (varsayÄ±lan 0.1 seÃ§ildi ) Ã¶ÄŸrenme katsayÄ±sÄ± * J_derivate deÄŸerini Ã§Ä±kararak yapmaktayÄ±z. Matematiksel olarak 

$$
Qj := Qj - a * ((1/m)*âˆ‘(hq(x)-y)*x  
$$

iÅŸlemini yapmakta. Burada J_derivate ile gerekli veriler dÄ±ÅŸÄ±nda k deÄŸerini de veriyoruz. Bu  k deÄŸeri tÃ¼rev ifadesinin ne olacaÄŸÄ±nÄ± belirlemektedir.

Bu iÅŸlem sonucunda Q[i] deÄŸeri â€œhatanÄ±n tÃ¼revi * alfaâ€ kadar miktar uzaklaÅŸÄ±p yakÄ±nlaÅŸmakta. Q[i+1] deÄŸerinde bu deÄŸiÅŸim hesaplanÄ±rken yine Q[i] kullanÄ±ldÄ±ÄŸÄ± iÃ§in deÄŸiÅŸim tÃ¼m Q[] deÄŸerlerinin deÄŸiÅŸim hesaplandÄ±ktan sonra yapÄ±lmaktadÄ±r.

Yani eÅŸzamanlÄ± olarak deÄŸiÅŸtirilmekte ve bÃ¶ylece hesaplama hatasÄ± oluÅŸmamaktadÄ±r. 

Son olarak ise her adÄ±mda maliyet bastÄ±rmak yerine her 50 adÄ±mda maliyet deÄŸeri bastÄ±rÄ±lmakta ve bu sonuÃ§lar tablo olarak Ã§izdirilmekte. 

Hemen alt satÄ±rda ise bu fonksiyon Ã§alÄ±ÅŸtÄ±rÄ±lmakta. FarklÄ± deÄŸerler iÃ§in sonuÃ§larÄ± Ã§alÄ±ÅŸtÄ±rÄ±p gÃ¶relim.

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2011.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2012.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2013.png)

epoch 8000 , alfa =0.005 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2014.png)

epoch 8000 , alfa =0.015 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2015.png)

epoch 8000 , alfa =0.35, iterasyon 3400

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2016.png)

epoch 8000 , alfa =0.35 , iterasyon = 8000

- Epoch 8000 ve alfa 0.35 iÃ§in Q deÄŸerlerimizi gÃ¶relim.

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2017.png)

- Son olarak eÄŸittiÄŸimiz modelimizden Q deÄŸerlerini bulduk. Åimdi bu deÄŸerleri x_test ve y_test Ã¼zerinden Q deÄŸerlerimizi deneyelim ve yÃ¼zde kaÃ§ hata yaptÄ±ÄŸÄ±mÄ±zÄ± her satÄ±r iÃ§in bulalÄ±m

Ã–ncelikle her x_test - y_test arasÄ±ndaki yÃ¼zdelik hatayÄ± bulacaÄŸÄ±mÄ±z iÃ§in bu deÄŸerleri bir liste iÃ§erisinde tutmalÄ±yÄ±z. Bu yÃ¼zden  hatalar listesi tanÄ±mladÄ±k 

ArdÄ±ndan degerlendirme fonksiyonunda her x_test satÄ±rÄ±nÄ± gÃ¶rmek iÃ§in dÃ¶ngÃ¼ yazdÄ±k. Bu dÃ¶ngÃ¼de gercek deÄŸiÅŸkenimiz y_test.values[i] ile o satÄ±rdaki, deÄŸerini bildiÄŸimiz y bilgisini tutacak. 

tahmin deÄŸiÅŸkeni ise hipotezimizi uygulayacak. 

Åimdi aradaki farkÄ±n mutlak deÄŸerini gercek veriye bÃ¶lÃ¼p 100 ile Ã§arparak 

$$
(|gercek - tahmin| / gercek )* 100
$$

aradaki yÃ¼zdelik farkÄ± bulmuÅŸ olduk. Bunu listemize ekleyebiliriz.

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2018.png)

DÃ¶ngÃ¼ bittiÄŸinde elimizde hatalarÄ±n hepsi bulunacak. BunlarÄ±n aritmetik ortalamasÄ±nÄ± alarak ortalamada ne kadar hatamÄ±z olduÄŸunu bulabiliriz. 

Bu hÃ¼creyi de Ã§alÄ±ÅŸtÄ±rÄ±rsak elde ettiÄŸimiz sonuÃ§lar ÅŸu ÅŸekilde ( Epoch 8000 ve alfa 0.35 iÃ§in ) 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2019.png)

<strong> Ortalama olarak %35 hata oranÄ±na sahibiz. </strong>

Åimdi 2. sorumuza geÃ§elim 

## ***b)Â VeriÂ setindekiÂ bÃ¼tÃ¼nÂ Ã¶znitelikleriÂ kullanarakÂ Ã§okÂ deÄŸiÅŸkenliÂ birÂ regresyonÂ modeliÂ ileÂ evinÂ fiyatÄ±nÄ±Â tahminÂ ediniz.***

Bu bÃ¶lÃ¼mde Ã¶nceki fonksiyon ve algoritmalarÄ± kullanacaÄŸÄ±mÄ±z iÃ§in detaya girilmeden genelleÅŸtirilmiÅŸ halleri anlatÄ±lacaktÄ±r. 

Ä°lk olarak verilerimizi X ve Y olarak tekrar parÃ§alÄ±yoruz. Burada evin fiyatÄ±nÄ± etkilemeyecek olan verileri ilik satÄ±rda atÄ±lmÄ±ÅŸtÄ±r.  Fakat Ã¶ncekinin aksine parametre sayÄ±sÄ±ndan baÄŸÄ±msÄ±z genelleÅŸmiÅŸ bir uygulama olacaÄŸÄ± iÃ§in eklenmesinde de tasarÄ±m aÃ§Ä±sÄ±ndan sorun yoktur. Sadece yavaÅŸlamaya neden olacaktÄ±r. 

Bu yÃ¼zden bu deÄŸerler atÄ±larak geri kalan 14 kolon attribute olarak seÃ§ildi. 

Hemen ardÄ±ndan veriler train ve test olarak %70 oranda parÃ§alandÄ±. 

Q deÄŸiÅŸkenlerimizi tutmak iÃ§in Q_all[ ] listesi oluÅŸturuldu.

Her iterasyonda bu Q_all[ *i* ] bilgisi deÄŸiÅŸeceÄŸi iÃ§in bu deÄŸiÅŸimler tek tek deÄŸiÅŸkenlerde tutulmasÄ± yanlÄ±ÅŸ olacaktÄ±r. Bu yÃ¼zden q_temp listesi oluÅŸturuldu.

 Bir dÃ¶ngÃ¼ ile kolon sayÄ±sÄ± + 1 kadar 1 deÄŸeri listelere eklendi. 

Bunun sebebi  x[*i*] kolon bilgilerine karÅŸÄ±lÄ±k gelmekteydi. Ã–rneÄŸin sqft_living gibi. Fakat hipotezimizi $Q0 + Q1x1 + Q2x2 + . . . + Qnxn$  ÅŸeklinde dÃ¼ÅŸÃ¼ndÃ¼k. Burada fazladan Q0 fazlalÄ±ÄŸÄ± bulunduÄŸu iÃ§in 15 adet 1 ekledik.

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2020.png)

- Åimdi verilerimizi Normalize edebiliriz.

Burada yaptÄ±ÄŸÄ±mÄ±z iÅŸlem ilk soruda yaptÄ±ÄŸÄ±mÄ±z ile aynÄ± 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2021.png)

- Ã–nceki soruda hipotezimizi hep baÅŸka fonksiyonlarÄ±n iÃ§erisinde yazmÄ±ÅŸtÄ±k. Åimdi ise bunu ayÄ±ralÄ±m

Hq ile hipotezimizi bulacaÄŸÄ±z. Bunu 

$$
Q0 + Q1x1 + Q2x2 + . . . + Qnxn
$$

iÅŸlemi ile tanÄ±mlamÄ±ÅŸtÄ±k. Bu yÃ¼zden bu toplamÄ± tutacak Ã¶nce hq deÄŸiÅŸkenimizi oluÅŸturuyoruz. ArdÄ±ndan her X satÄ±rÄ± iÃ§in bu formÃ¼lÃ¼ iÅŸletiyoruz. Burada for dÃ¶ngÃ¼sÃ¼nde indisleri de kullanmak istediÄŸimiz iÃ§in enumerate ile Xâ€™leri key value haline getirdik.

AyrÄ±ca Q[0] verimizin fazlalÄ±k olduÄŸunu ve x Ã§arpanÄ± olmadÄ±ÄŸÄ±nÄ± belirtmiÅŸtik. Bu yÃ¼zden iterasyonu 1â€™den baÅŸlatÄ±yoruz. 

Q[0] â€˜Ä± ise en sonda sonucumuza ekleyip deÄŸeri dÃ¶ndÃ¼rÃ¼yoruz. 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2022.png)

### Maliyet Fonksiyonu - 2

- ArtÄ±k daha genel maliyet ve gradiant fonksiyonlarÄ± yazacaÄŸÄ±mÄ±z iÃ§in bazÄ± bÃ¶lÄŸmleri deÄŸiÅŸtireceÄŸiz.

Burada satÄ±r sayÄ±mÄ±zÄ± aldÄ±ktan sonra toplam ve result deÄŸiÅŸkenleri belirliyoruz. 

Burada tekrar enumerate kullanarak X iÃ§erisindeki satÄ±rlara ulaÅŸÄ±yoruz ve her satÄ±rÄ± sayabilir oluyoruz. 

ArtÄ±k ana iÅŸlemimiz olan farkÄ±n karelerini toplama iÅŸlemini yapabiliriz. 

Burada satÄ±r bilgisi elimizde olduÄŸu iÃ§in ( 14 attributeâ€™a sahip 1 tane X bilgisi  ) bunu ve Q bilgilerini Hq fonksiyonumuza verip tahmini y Ã§Ä±ktÄ±sÄ±nÄ± buluyoruz. 

Bunu GerÃ§ek Y ile Ã§Ä±kartÄ±p karesini alÄ±yoruz ve sonuÃ§larÄ± topluyoruz

En sonda bu toplamÄ± 2*satÄ±r sayÄ±sÄ±na bÃ¶lÃ¼yoruz. 

BurasÄ± gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z maliyet fonksiyonunun aynÄ±sÄ±. 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2023.png)

- Åimdi J_derivate ile tÃ¼revli halini gÃ¶relim

Burada ise yine aynÄ± iÅŸlemleri yapÄ±yor , Hq ile hipotezi hesaplayÄ±p farkÄ± buluyoruz. Ã–ncesinde yaptÄ±ÄŸÄ±mÄ±z gibi yine X[i][k-1] katsayÄ±sÄ± ile Ã§arpÄ±yoruz. (O Q[i] deÄŸerinin Ã¶nÃ¼ndeki x attributeâ€™u ) 

Ve en sonda toplam deÄŸeri satÄ±r sayÄ±sÄ±na bÃ¶lÃ¼p dÃ¶ndÃ¼rÃ¼yoruz. 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2024.png)

### GradientDescent AlgoritmasÄ±

- Modelimizi eÄŸitme aÅŸamasÄ±nÄ± genel bir ÅŸekilde yazmaya geÃ§ebiliriz.

Burada ilk olarak copy kÃ¼tÃ¼phanesini import ettik. Yeri geldiÄŸinde bahsedilecek.

ArdÄ±ndan cost_all listesi oluÅŸturuldu. Bu liste ile her iterasyonda maliyetimizi bu listeye ekleyeceÄŸiz. BÃ¶ylece listedeki verileri ve liste uzunluÄŸunu kullanarak matplotlibâ€™e tablo oluÅŸturtacaÄŸÄ±z. 

ArdÄ±ndan dÃ¶ngÃ¼ ile verilen epoch sayÄ±sÄ± kadar modelimizi eÄŸiticez. 

Her dÃ¶ngÃ¼ iÃ§erisinde temp_cost ile maliyeti hesaplamaktayÄ±z ve bu veriyi cost_all  listemize eklemekteyiz. 

ArdÄ±ndan bir dÃ¶ngÃ¼ ile modelin eÄŸitildiÄŸi kÄ±smÄ± yapmaktayÄ±z. Burada kolon sayÄ±sÄ±nÄ±n bir fazlasÄ± kadar dÃ¶ngÃ¼ oluÅŸturduk. Ã‡Ã¼nkÃ¼ baÅŸta da belirttiÄŸimiz gibi Q[0] yanÄ±nda X Ã§arpanÄ± bulunmamakta.

Åimdi iÃ§erideki her iterasyonda Q[i] deÄŸerinin ne kadar deÄŸiÅŸtiÄŸi Ã¶nceki soru ile benzer bir ÅŸekilde J_derivate_2 ile hesaplanacak. ArdÄ±ndan Ã¶nceki sorudan farklÄ± olarak bu geÃ§ici sonuÃ§ bir listede tutulacak. Bu iÅŸlem her Q deÄŸeri iÃ§in yapÄ±ldÄ±ktan sonra iÃ§erideki dÃ¶ngÃ¼mÃ¼z bitmiÅŸ olacak. 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2025.png)

DÃ¶ngÃ¼ sonlandÄ±ktan sonra Q listemizi bulduÄŸumuz geÃ§ici q_temp ile deÄŸiÅŸtirebiliriz. Fakat burada  Q_all = q_temp yaparsak python liste Ã¶zelliÄŸinden dolayÄ± shallow copy yapmÄ±ÅŸ olacaÄŸÄ±z ve referans kopyalanmasÄ± sorunu oluÅŸacak. Bu yÃ¼zden import ettiÄŸimiz copy ile deepcopy metodunu kullanarak deÄŸerleri kopyalÄ±yoruz. 

Bu iterasyonu her i deÄŸeri iÃ§in gÃ¶rmek uzun bir sÃ¼reÃ§ olacaÄŸÄ± iÃ§in her 50 adÄ±mda sonucu 

gÃ¶receÄŸimiz bir ifade ekliyoruz. Burada ayrÄ±ca plt.scatter ile cost_all listemizi verip maliyetimizin nasÄ±l bir grafik izlediÄŸini de Ã§izdiriyoruz.

SonuÃ§larÄ± ve eÄŸitmeyi en sonda gÃ¶relim 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2026.png)

- Åimdi ilk soruda yaptÄ±ÄŸÄ±mÄ±z yÃ¼zdelik hata bulmayÄ± revize edelim ve tekrar deneyelim.

Burada hatalarÄ± tutacak bir listemiz var. Bu listeye her iterasyonda gerÃ§ek deÄŸer ile tahmin deÄŸer arasÄ±ndaki yÃ¼zdelik farkÄ± buluyoruz. Tahmini ise x_test ve Q deÄŸerlerini hipotez fonksiyonumuza vererek yaptÄ±k. 

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2027.png)

Åimdi sonuÃ§larÄ±mÄ±zÄ± gÃ¶relim.

### (alfa=0.20 , epoch=1000)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2028.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2029.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2030.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2031.png)

Q deÄŸerleri = [-154535.40088235194, -23601.05417902612, 382231.68690385355, 521983.1666968419, -8862.907305400215, 132577.3125777927, 471541.3542879963, 233011.74856392056, 69281.84846283196, 959837.6930524483, 592346.8325346287, 367966.8133860462, -404452.1103062461, 463869.4666701383, -40863.99019222211]

### %â€™lik hata

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2032.png)

### (alfa=0.005 , epoch=1000)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2033.png)

### %lik hata

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2034.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2035.png)

![Untitled](Proje-1%20Regresyon%2008c0dec28ff349439db3dfe26f1b0b5d/Untitled%2036.png)

# SonuÃ§ :

<strong>
Veriler x_train ve y_train ile eÄŸitilmiÅŸ ve x_test ve y_test ile maliyet deÄŸerleri kontrol edilip , hata oranlarÄ± bulunmuÅŸtur.

Ä°lk soruda 2 parametre iÃ§in alfa ve epoch deÄŸerleri ile oynanarak %30 - % 50 arasÄ±nda hata oranlarÄ± alÄ±nmÄ±ÅŸtÄ±r

Fakat 2. soruda tÃ¼m parametrelerin verilmesinde hata oranlarÄ± %99.99 oranÄ±nda olmuÅŸtur.
</strong>
